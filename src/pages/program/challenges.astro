---
import Heading3 from "../../components/Tags/Heading3.astro";
import Heading4 from "../../components/Tags/Heading4.astro";
import List from "../../components/Tags/List.astro";
import MainHeader from "../../components/Tags/MainHeader.astro";
import Paragraph from "../../components/Tags/Paragraph.astro";
import Section from "../../components/Tags/Section.astro";
import UList from "../../components/Tags/UList.astro";
import Layout from "../../layouts/Layout.astro";
---

<Layout title="Program :: AVSS2023">
  <MainHeader metaTitle="Program" title="Challenges" />
  <Section>
    <Heading3 title="Cross-view Multi-object Tracking in DIVerse Open Scenes" />
    <Heading4 title="Introduction" />
    <Paragraph>
      Cross-view multi-object tracking is a challenging problem in computer
      vision that involves tracking multiple objects of interest across multiple
      camera views. The goal is to associate object detections across different
      views and time frames, and to maintain the identity of each object
      throughout the tracking process. Interested participants are invited to
      apply their approaches and methods on a novel cross-view multi-object
      tracking dataset <strong>DIVOTrack</strong> being made available by the challenge
      organizers. We collect data in 15 different realÂ­world scenarios, including
      indoor and outdoor public scenes. The dataset is split into development data
      with a training set, testing set, and challenge set. Each of them contains
      5 different real-world scenarios, including indoor and outdoor public scenes.
      The duration of each video is about 1 minute at 30FPS. All the sequences are
      captured by using three moving cameras and are manually synchronized. There
      are both moving dense crowds and sparse pedestrians in outdoor scenes. The
      surrounding environment of outdoor scenes is diverse, including streets, vehicles,
      buildings, and public infrastructures. Meanwhile, the indoor scene comes from
      a large shopping mall, with a more complicated and severe occlusion of the
      crowd than the outdoor environment. The development data includes original
      video clips, object bounding boxes, and global id for each object. The organizers
      will support the evaluation and scoring the result of the challenge set. The
      results of each scene has 3 txt files. The txt file format is <code
        >frame_id</code
      >
      <code>person_id</code>
      <code>Ix</code>
      <code>ly</code>
      <code>w</code>
      <code>h</code> , <strong>fid</strong> is frame id, <strong>pid</strong> is
      person id, <strong>Ix</strong> is the x coordinate of the lefttop position,
      <strong>ly</strong> is the y coordinate of the lefttop position, <strong
        >w</strong
      > and <strong>h</strong> are the width and height of each person box. The participants
      can download the <strong>DIVOTrack</strong> from <a
        class="fancy"
        href="
      https://github.com/shengyuhao/DIVOTrack#dataset-downloads"
      >
        here</a
      >, and use the evaluation protocol from <a
        class="fancy"
        href="https://github.com/shengyuhao/DIVOTrack/tree/main/MOTChallengeEvalKit_cv_test/cv_test"
        >here</a
      >
    </Paragraph>
    <Heading4 title="The rules for participation" />
    <Paragraph>
      The competition is open to everyone. But the members from the teams of the
      organizers cannot join. All teams should mailed the license for using <strong
        >DIVOTrack</strong
      > to shengyuhao (<a class="fancy" href="mailto:shengyuhao@zju.edu.cn"
        >shengyuhao@zju.edu.cn</a
      >). <strong
        >The top 3 teams will be invited to give a presentation, including
        methods and experimental results.</strong
      >
    </Paragraph>
    <Paragraph>
      Note: Please use your <strong>institutional</strong> email to register for
      the competition, otherwise the registration will not be approved.
    </Paragraph>
    <Heading4
      title="The criteria that will be used to evaluate the submitted entries"
    />
    <Paragraph>
      The evaluation should be user-friendly and convenient for participants. It
      should also be fair and safe to be hacked. We designed detailed rules as
      follows:
    </Paragraph>
    <UList>
      <List>We will limit the number of submissions each day to 1.</List>
      <List>
        Run submission files should be emailed directly to shengyuhao (<a
          class="fancy"
          href="mailto:shengyuhao@zju.edu.cn">shengyuhao@zju.edu.cn</a
        >). The ranking will be updated on the scoreboard.
      </List>
      <List>
        The top 3 teams in the final scoreboard need to send their programs to
        the organizers. The programs are being run to reproduce their results.
      </List>
    </UList>
    <Heading4 title="The team that will run the challenge" />
    <Paragraph>
      The team running the challenge from Zhejiang University - University of
      Illinois Urbana-Champaign Institute. The team would be responsible for
      creating the challenge dataset, defining the evaluation metrics and rules,
      and providing support and guidance to the participants throughout the
      challenge.
    </Paragraph>
    <Heading4 title="Important dates" />
    <UList>
      <List>Baseline release: Available now</List>
      <List>
        Training and Testing dataset release: Available now, 2023 [Available now
        after submitting the data agreement from located <strong>HERE</strong>]
      </List>
      <List>Challenge set release: May 15, 2023</List>
      <List>Submissions of solutions to organizers: October 24, 2023</List>
      <List>Competition results announcement: November 1, 2023</List>
      <List>Grand Challenge at AVSS 2023: TBD</List>
    </UList>
  </Section>
</Layout>
